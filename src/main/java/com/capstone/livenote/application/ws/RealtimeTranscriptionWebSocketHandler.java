package com.capstone.livenote.application.ws;

import com.capstone.livenote.domain.lecture.entity.Lecture;
import com.capstone.livenote.domain.lecture.repository.LectureRepository;
import com.capstone.livenote.domain.transcript.entity.Transcript;
import com.capstone.livenote.domain.transcript.repository.TranscriptRepository;
import com.capstone.livenote.domain.transcript.service.TranscriptService;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;
import org.springframework.web.socket.BinaryMessage;
import org.springframework.web.socket.CloseStatus;
import org.springframework.web.socket.TextMessage;
import org.springframework.web.socket.WebSocketSession;
import org.springframework.web.socket.handler.AbstractWebSocketHandler;

import java.net.URI;
import java.net.http.HttpClient;
import java.net.http.WebSocket;
import java.nio.ByteBuffer;
import java.time.Duration;
import java.util.Base64;
import java.util.Map;
import java.util.Optional;
import java.util.Queue;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.CompletionStage;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * ÌîÑÎ°†Ìä∏ ‚Üí Î∞±ÏóîÎìú Î∞îÏù¥ÎÑàÎ¶¨ PCM(24kHz, mono) Ï†ÑÏÜ°ÏùÑ Î∞õÏïÑ
 * OpenAI Realtime(gpt-4o-transcribe)Î°ú Ï§ëÍ≥ÑÌïòÎäî WebSocket Ìï∏Îì§Îü¨.
 *
 * ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏöîÏ≤≠: /ws/transcription?sessionId=<lectureId>
 * - BinaryMessageÎßå Ï†ÑÏÜ°(PCM16). ÏÑúÎ≤ÑÍ∞Ä Base64 Ïù∏ÏΩîÎî© ÌõÑ input_audio_buffer.appendÎ°ú Ï†ÑÎã¨.
 * - OpenAI Í≤∞Í≥º Ï§ë transcript Í≥ÑÏó¥ Ïù¥Î≤§Ìä∏Î•º {type:"transcript", data:{content,isFinal}}Î°ú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Ï†ÑÎã¨.
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class RealtimeTranscriptionWebSocketHandler extends AbstractWebSocketHandler {

    private static final URI OPENAI_REALTIME_URI =
            URI.create("wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01");

    private final LectureRepository lectureRepository;
    private final TranscriptRepository transcriptRepository;
    private final TranscriptService transcriptService;
    private final ObjectMapper objectMapper;

    @Value("${OPENAI_API_KEY}")
    private String openAiApiKey;

    private final Map<String, SessionContext> contexts = new ConcurrentHashMap<>();

    @Override
    public void afterConnectionEstablished(WebSocketSession session) {
        String lectureIdStr = getQueryParam(session, "sessionId");
        if (lectureIdStr == null) {
            sendErrorAndClose(session, "sessionId is required");
            return;
        }

        Long lectureId;
        try {
            lectureId = Long.parseLong(lectureIdStr);
        } catch (NumberFormatException e) {
            sendErrorAndClose(session, "sessionId must be a number");
            return;
        }

        Optional<Lecture> lectureOpt = lectureRepository.findById(lectureId);
        if (lectureOpt.isEmpty()) {
            sendErrorAndClose(session, "lecture not found");
            return;
        }
        Lecture lecture = lectureOpt.get();
        String language = lecture.getSttLanguage() == null ? "ko" : lecture.getSttLanguage();

        // Í∞ïÏùò Ïû¨Í∞ú Ïãú Ïù¥Ï†Ñ ÏµúÎåÄ endSec Í∞ÄÏ†∏Ïò§Í∏∞
        Integer maxEndSec = transcriptRepository.findMaxEndSecByLectureId(lectureId);
        int startFromSec = maxEndSec != null ? maxEndSec : 0;
        
        log.info("[RealtimeWS] Lecture resume info: lectureId={} startFromSec={}", lectureId, startFromSec);

        SessionContext ctx = new SessionContext(session, lectureId, language, startFromSec);
        contexts.put(session.getId(), ctx);

        connectOpenAi(ctx);
        log.info("[RealtimeWS] connected clientSessionId={} lectureId={} lang={} resumeFrom={}s",
                session.getId(), lectureId, language, startFromSec);
    }

    @Override
    protected void handleBinaryMessage(WebSocketSession session, BinaryMessage message) {
        SessionContext ctx = contexts.get(session.getId());
        if (ctx == null) {
            sendErrorAndClose(session, "session context missing");
            return;
        }
        byte[] bytes = toByteArray(message.getPayload());
        if (bytes.length == 0) {
            return;
        }
        ctx.enqueueAudio(bytes);
    }

    @Override
    public void handleTransportError(WebSocketSession session, Throwable exception) {
        log.warn("[RealtimeWS] transport error sessionId={} err={}", session.getId(), exception.getMessage());
        sendError(session, "transport error: " + exception.getMessage());
        closeSession(session, CloseStatus.SERVER_ERROR);
    }

    @Override
    public void afterConnectionClosed(WebSocketSession session, CloseStatus status) {
        SessionContext ctx = contexts.remove(session.getId());
        if (ctx != null) {
            ctx.close();
        }
        log.info("[RealtimeWS] closed sessionId={} code={}", session.getId(), status);
    }

    private void connectOpenAi(SessionContext ctx) {
        HttpClient client = HttpClient.newBuilder()
                .connectTimeout(Duration.ofSeconds(5))
                .build();

        CompletableFuture<WebSocket> future = client.newWebSocketBuilder()
                .header("Authorization", "Bearer " + openAiApiKey)
                .header("OpenAI-Beta", "realtime=v1")
                .connectTimeout(Duration.ofSeconds(5))
                .buildAsync(OPENAI_REALTIME_URI, new OpenAiListener(ctx));

        future.whenComplete((ws, err) -> {
            if (err != null) {
                log.error("[RealtimeWS] OpenAI connect fail lectureId={} err={}", ctx.lectureId, err.getMessage());
                sendError(ctx.clientSession, "failed to connect OpenAI: " + err.getMessage());
                closeSession(ctx.clientSession, CloseStatus.SERVER_ERROR);
                return;
            }
            ctx.setOpenAiWebSocket(ws);
            log.info("[RealtimeWS] OpenAI connected lectureId={}", ctx.lectureId);
            ctx.flushPending();
        });
    }

    private void sendTranscript(SessionContext ctx, String content, boolean isFinal) {
        try {
            // 1. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î°ú Ï†ÑÏÜ°
            String json = objectMapper.writeValueAsString(
                    Map.of(
                            "type", "transcript",
                            "data", Map.of(
                                    "content", content,
                                    "isFinal", isFinal
                            )
                    )
            );
            ctx.clientSession.sendMessage(new TextMessage(json));
            
            // 2. isFinal==trueÏùº Îïå TranscriptServiceÎ•º ÌÜµÌï¥ Ï†ÄÏû• (ÏöîÏïΩ ÏÉùÏÑ± Ìä∏Î¶¨Í±∞)
            if (isFinal && content != null && !content.trim().isEmpty()) {
                // Î∞úÌôî Íµ¨Í∞Ñ Í∏∏Ïù¥(Ï¥à): speech_started ~ ÌòÑÏû¨ÍπåÏßÄ
                int segmentSec = ctx.endActiveSegment();
                if (segmentSec <= 0) segmentSec = Math.max(1, content.trim().length() / 10); // ÏµúÏÜå 1Ï¥à Î≥¥Ï†ï

                int startSec = ctx.lastTranscriptEndSec;
                int endSec = startSec + segmentSec;

                // TranscriptServiceÎ•º ÌÜµÌï¥ Ï†ÄÏû• -> SectionAggregationService.onNewTranscript() ÏûêÎèô Ìò∏Ï∂ú
                transcriptService.saveFromStt(ctx.lectureId, startSec, endSec, content.trim());

                ctx.lastTranscriptEndSec = endSec;
                ctx.spokenSeconds = endSec; // spoken ÏãúÍ∞Ñ ÎàÑÏ†Å

                log.info("[RealtimeWS] ‚úÖ Transcript saved to DB via TranscriptService: lectureId={} startSec={} endSec={} text={}",
                        ctx.lectureId, startSec, endSec, 
                        content.length() > 50 ? content.substring(0, 50) + "..." : content);
            }
        } catch (Exception e) {
            log.warn("[RealtimeWS] send transcript failed sessionId={} err={}", ctx.clientSession.getId(), e.getMessage());
        }
    }

    private void sendError(WebSocketSession session, String message) {
        try {
            String json = objectMapper.writeValueAsString(
                    Map.of("type", "error", "data", Map.of("error", message))
            );
            session.sendMessage(new TextMessage(json));
        } catch (Exception ignored) {
        }
    }

    private void sendErrorAndClose(WebSocketSession session, String message) {
        sendError(session, message);
        closeSession(session, CloseStatus.BAD_DATA);
    }

    private void closeSession(WebSocketSession session, CloseStatus status) {
        try {
            session.close(status);
        } catch (Exception ignored) {
        }
    }

    private String getQueryParam(WebSocketSession session, String key) {
        URI uri = session.getUri();
        if (uri == null || uri.getQuery() == null) return null;
        String[] pairs = uri.getQuery().split("&");
        for (String pair : pairs) {
            String[] kv = pair.split("=", 2);
            if (kv.length == 2 && kv[0].equals(key)) {
                return kv[1];
            }
        }
        return null;
    }

    private byte[] toByteArray(ByteBuffer buffer) {
        byte[] arr = new byte[buffer.remaining()];
        buffer.get(arr);
        return arr;
    }

    private class OpenAiListener implements WebSocket.Listener {
        private final SessionContext ctx;

        OpenAiListener(SessionContext ctx) {
            this.ctx = ctx;
        }

        @Override
        public void onOpen(WebSocket webSocket) {
            // OpenAI Realtime ÏÑ∏ÏÖò ÏÑ§Ï†ï: Ïò§ÎîîÏò§ ÏûÖÎ†•, ÌÖçÏä§Ìä∏ Ï∂úÎ†•, VAD ÌôúÏÑ±Ìôî
            try {
                String sessionConfig = """
                    {
                      "type": "session.update",
                      "session": {
                        "modalities": ["text"],
                        "instructions": "You are a helpful assistant that transcribes audio to text. Respond with transcriptions only. Don't use chinese and japanese and spanish when language setting is ko",
                        "voice": "alloy",
                        "input_audio_format": "pcm16",
                        "output_audio_format": "pcm16",
                        "input_audio_transcription": {
                          "model": "whisper-1"
                        },
                        "turn_detection": {
                          "type": "server_vad",
                          "threshold": 0.5,
                          "prefix_padding_ms": 300,
                          "silence_duration_ms": 500
                        },
                        "tools": [],
                        "tool_choice": "none",
                        "temperature": 0.8,
                        "max_response_output_tokens": "inf"
                      }
                    }
                    """;
                webSocket.sendText(sessionConfig, true);
                log.info("[RealtimeWS] Sent session.update with audio input enabled");
            } catch (Exception e) {
                log.warn("[RealtimeWS] Failed to send session config: {}", e.getMessage());
            }
            webSocket.request(1);
        }

        @Override
        public CompletionStage<?> onText(WebSocket webSocket, CharSequence data, boolean last) {
            handleOpenAiText(ctx, data.toString());
            webSocket.request(1);
            return null;
        }

        @Override
        public CompletionStage<?> onBinary(WebSocket webSocket, ByteBuffer data, boolean last) {
            webSocket.request(1);
            return null; // ÏòàÏÉÅÏπò Î™ªÌïú Î∞îÏù¥ÎÑàÎ¶¨Îäî Î¨¥Ïãú
        }

        @Override
        public void onError(WebSocket webSocket, Throwable error) {
            log.warn("[RealtimeWS] OpenAI error lectureId={} err={}", ctx.lectureId, error.getMessage());
            sendError(ctx.clientSession, "openai error: " + error.getMessage());
        }

        @Override
        public CompletionStage<?> onClose(WebSocket webSocket, int statusCode, String reason) {
            log.info("[RealtimeWS] OpenAI closed lectureId={} code={} reason={}", ctx.lectureId, statusCode, reason);
            return null;
        }
    }

    private void handleOpenAiText(SessionContext ctx, String text) {
        try {
            JsonNode root = objectMapper.readTree(text);
            String type = root.path("type").asText("");
            
            // ÎîîÎ≤ÑÍπÖÏö© Î°úÍ∑∏ Ï∂îÍ∞Ä
            log.debug("[RealtimeWS] Received event type: {} lectureId={}", type, ctx.lectureId);

            // Ïò§Î•ò Ïù¥Î≤§Ìä∏
            if (type.equalsIgnoreCase("error") || root.has("error")) {
                String errMsg = root.path("error").path("message").asText(type.isEmpty() ? "openai error" : type);
                log.error("[RealtimeWS] OpenAI error: {} lectureId={}", errMsg, ctx.lectureId);
                sendError(ctx.clientSession, errMsg);
                return;
            }
            
            // input_audio_buffer.speech_started Ïù¥Î≤§Ìä∏
            if (type.equals("input_audio_buffer.speech_started")) {
                log.info("[RealtimeWS] Speech started lectureId={}", ctx.lectureId);
                ctx.transcriptBuffer.setLength(0); // Î≤ÑÌçº Ï¥àÍ∏∞Ìôî
                ctx.speechStartMillis = System.currentTimeMillis();
                return;
            }
            
            // conversation.item.input_audio_transcription.completed Ïù¥Î≤§Ìä∏ (Whisper Ï†ÑÏÇ¨ Í≤∞Í≥º)
            if (type.equals("conversation.item.input_audio_transcription.completed")) {
                String transcript = root.path("transcript").asText("");
                if (!transcript.isEmpty()) {
                    log.info("[RealtimeWS] Transcription completed: {} lectureId={}", transcript, ctx.lectureId);
                    sendTranscript(ctx, transcript, true);
                    ctx.transcriptBuffer.setLength(0);
                }
                return;
            }
            
            // conversation.item.input_audio_transcription.failed Ïù¥Î≤§Ìä∏
            if (type.equals("conversation.item.input_audio_transcription.failed")) {
                JsonNode error = root.path("error");
                log.warn("[RealtimeWS] Transcription failed: {} lectureId={}", error, ctx.lectureId);
                return;
            }

            // delta / done Ï≤òÎ¶¨ (Í∞ÄÎä•Ìïú Ïù¥Î≤§Ìä∏ Îã§ÏñëÏÑ±ÏùÑ Ïª§Î≤Ñ)
            if (type.contains("output_text.delta") || type.contains("audio_transcript.delta")) {
                String delta = extractDelta(root);
                if (!delta.isEmpty()) {
                    ctx.transcriptBuffer.append(delta);
                    sendTranscript(ctx, ctx.transcriptBuffer.toString(), false);
                }
                return;
            }

            if (type.contains("output_text.done") || type.contains("audio_transcript.done") || type.contains("response.done")) {
                String finalText = ctx.transcriptBuffer.toString();
                if (!finalText.isEmpty()) {
                    sendTranscript(ctx, finalText, true);
                    ctx.transcriptBuffer.setLength(0);
                }
                return;
            }

            // Í∏∞ÌÉÄ ÌÉÄÏûÖ: response.output_text ÌòπÏùÄ transcriptÍ∞Ä Î∞îÎ°ú contentÏóê ÏûàÏùÑ Í≤ΩÏö∞
            if (root.has("content")) {
                String content = root.path("content").asText("");
                if (!content.isEmpty()) {
                    ctx.transcriptBuffer.append(content);
                    sendTranscript(ctx, ctx.transcriptBuffer.toString(), true);
                    ctx.transcriptBuffer.setLength(0);
                }
            }

            // Ï†ÑÏÇ¨ ÏôÑÎ£å Ïù¥Î≤§Ìä∏
            if (type.equals("conversation.item.input_audio_transcription.completed")) {
                String transcript = root.path("transcript").asText("").trim();
                if (!transcript.isEmpty()) {
                    log.info("üé§ [RealtimeWS] Transcribed: {}", transcript);
                    sendTranscript(ctx, transcript, true); // DB Ï†ÄÏû• Ìä∏Î¶¨Í±∞
                }
                return;
            }

            // Ïã§ÏãúÍ∞Ñ Î∂ÄÎ∂Ñ Ï†ÑÏÇ¨ (User ExperienceÏö©)
            if (type.equals("response.audio_transcript.delta")) {
                String delta = root.path("delta").asText("");
                if (!delta.isEmpty()) {
                    sendTranscript(ctx, delta, false); // ÌîÑÎ°†Ìä∏ÏóêÎßå Î≥¥Ïó¨Ï§å (Ï†ÄÏû• X)
                }
                return;
            }

        } catch (Exception e) {
            log.warn("[RealtimeWS] parse OpenAI text failed lectureId={} err={}", ctx.lectureId, e.getMessage());
        }
    }

    private String extractDelta(JsonNode root) {
        // Ïö∞ÏÑ†ÏàúÏúÑ: delta ÌïÑÎìú, text ÌïÑÎìú, content Î∞∞Ïó¥
        if (root.has("delta")) {
            JsonNode delta = root.get("delta");
            if (delta.isTextual()) return delta.asText("");
        }
        if (root.has("text") && root.get("text").isTextual()) {
            return root.get("text").asText("");
        }
        if (root.has("content") && root.get("content").isArray()) {
            StringBuilder sb = new StringBuilder();
            root.get("content").forEach(n -> {
                if (n.isTextual()) sb.append(n.asText());
            });
            return sb.toString();
        }
        return "";
    }

    private static class SessionContext {
        private final WebSocketSession clientSession;
        private final Long lectureId;
        private final String language;
        private final Queue<byte[]> pendingAudio = new ConcurrentLinkedQueue<>();
        private final AtomicBoolean openAiReady = new AtomicBoolean(false);
        private final StringBuilder transcriptBuffer = new StringBuilder();
        private volatile WebSocket openAiWebSocket;
        private final long startTimeMillis = System.currentTimeMillis();
        private final int baseSeconds; // Ïû¨Í∞ú Ïãú Í∏∞Ï§Ä ÏãúÍ∞Ñ(Ï¥à)
        private int lastTranscriptEndSec;
        private long speechStartMillis = -1;
        private int spokenSeconds; // ÎßêÌïú ÏãúÍ∞Ñ ÎàÑÏ†Å

        SessionContext(WebSocketSession clientSession, Long lectureId, String language, int startFromSec) {
            this.clientSession = clientSession;
            this.lectureId = lectureId;
            this.language = language;
            this.baseSeconds = startFromSec;
            this.lastTranscriptEndSec = startFromSec;
            this.spokenSeconds = startFromSec;
        }
        
        int getElapsedSeconds() {
            int active = 0;
            if (speechStartMillis > 0) {
                active = (int) ((System.currentTimeMillis() - speechStartMillis) / 1000);
            }
            return spokenSeconds + active;
        }

        void setOpenAiWebSocket(WebSocket ws) {
            this.openAiWebSocket = ws;
            openAiReady.set(true);
        }

        void enqueueAudio(byte[] bytes) {
            if (openAiReady.get() && openAiWebSocket != null) {
                sendAudio(bytes);
            } else {
                pendingAudio.add(bytes);
            }
        }

        void flushPending() {
            if (!openAiReady.get() || openAiWebSocket == null) return;
            byte[] data;
            while ((data = pendingAudio.poll()) != null) {
                sendAudio(data);
            }
        }

        void sendAudio(byte[] bytes) {
            if (openAiWebSocket == null) return;
            try {
                String b64 = Base64.getEncoder().encodeToString(bytes);
                String payload = "{\"type\":\"input_audio_buffer.append\",\"audio\":\"" + b64 + "\"}";
                openAiWebSocket.sendText(payload, true);
            } catch (Exception e) {
                // ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê ÏóêÎü¨ Ï†ÑÌååÎäî ÏÉÅÏúÑÏóêÏÑú Ï≤òÎ¶¨
            }
        }

        void sendToOpenAi(String payload) {
            if (openAiWebSocket != null) {
                openAiWebSocket.sendText(payload, true);
            }
        }

        void close() {
            if (openAiWebSocket != null) {
                try { openAiWebSocket.sendClose(WebSocket.NORMAL_CLOSURE, "client closed"); } catch (Exception ignored) {}
            }
        }

        int endActiveSegment() {
            if (speechStartMillis <= 0) return 0;
            long now = System.currentTimeMillis();
            double deltaSeconds = (now - speechStartMillis) / 1000.0;
            // ÏïΩÍ∞ÑÏùò ÏôÑÏ∂©(+1.5Ï¥à) ÌõÑ Ïò¨Î¶ºÌïòÏó¨ Î∞òÏò¨Î¶º Ìö®Í≥º
            int delta = (int) Math.ceil(deltaSeconds + 1.5);
            spokenSeconds += delta;
            speechStartMillis = -1;
            return delta;
        }
    }
}
