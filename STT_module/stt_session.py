"""
OpenAI Realtime API ì„¸ì…˜ ê´€ë¦¬
ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì„¸ì…˜ ë° ì´ë²¤íŠ¸ ì²˜ë¦¬
"""
import json
import time
import asyncio
import logging
from typing import Optional
from datetime import datetime
from fastapi import WebSocket
import websockets
from websockets.client import WebSocketClientProtocol

from config import (
    OPENAI_API_KEY,
    REALTIME_API_URL,
    TRANSCRIPTION_MODEL,
    LANGUAGE,
    VAD_ENABLED,
    VAD_THRESHOLD,
    VAD_PREFIX_PADDING_MS,
    VAD_SILENCE_DURATION_MS
)

logger = logging.getLogger(__name__)


class RealtimeSTTSession:
    """OpenAI Realtime API ì„¸ì…˜ ê´€ë¦¬"""
    
    def __init__(self, client_ws: WebSocket):
        self.client_ws = client_ws
        self.openai_ws: Optional[WebSocketClientProtocol] = None
        self.is_connected = False
        self.session_id = None
        self.heartbeat_task = None
        
        # ğŸ“Š ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        self.first_audio_time = None  # ì²« ì˜¤ë””ì˜¤ ì „ì†¡ ì‹œê°
        self.first_response_time = None  # ì²« ì „ì‚¬ ë¸íƒ€ ìˆ˜ì‹  ì‹œê°
        self.last_audio_time = None  # ë§ˆì§€ë§‰ ì˜¤ë””ì˜¤ ì „ì†¡ ì‹œê°
        self.transcripts = []  # ì „ì‚¬ ì™„ë£Œ ì´ë²¤íŠ¸ ê¸°ë¡ (ì‹œê°, í…ìŠ¤íŠ¸)
        
    async def connect_to_openai(self):
        """OpenAI Realtime APIì— ì—°ê²°"""
        try:
            headers = [
                ("Authorization", f"Bearer {OPENAI_API_KEY}"),
                ("OpenAI-Beta", "realtime=v1")
            ]
            
            self.openai_ws = await websockets.connect(
                REALTIME_API_URL,
                additional_headers=headers,
                ping_interval=20,  # 20ì´ˆë§ˆë‹¤ ping
                ping_timeout=10
            )
            
            self.is_connected = True
            logger.info("âœ… OpenAI Realtime API ì—°ê²° ì„±ê³µ")
            
            # ì„¸ì…˜ ì„¤ì •
            await self.configure_session()
            
            # heartbeat ì‹œì‘
            self.heartbeat_task = asyncio.create_task(self.heartbeat())
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ OpenAI Realtime API ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    async def configure_session(self):
        """ì„¸ì…˜ ì„¤ì • (í•œêµ­ì–´, VAD ë“±)"""
        config = {
            "type": "session.update",
            "session": {
                "modalities": ["text"],  # ğŸ”¥ ì˜¤ë””ì˜¤ ì¶œë ¥ ë¹„í™œì„±í™” (ë¹„ìš© ì ˆê°)
                "instructions": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ìŒì„± ì¸ì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê²Œ í•œêµ­ì–´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì„¸ìš”.",
                "input_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": TRANSCRIPTION_MODEL,
                    "language": LANGUAGE
                },
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": VAD_THRESHOLD,
                    "prefix_padding_ms": VAD_PREFIX_PADDING_MS,
                    "silence_duration_ms": VAD_SILENCE_DURATION_MS
                } if VAD_ENABLED else None,
                "temperature": 0.6,  # Realtime API ìµœì†Œê°’
            }
        }
        
        await self.openai_ws.send(json.dumps(config))
        logger.info(f"âš™ï¸ ì„¸ì…˜ ì„¤ì • ì™„ë£Œ (ì–¸ì–´: {LANGUAGE}, VAD: {VAD_ENABLED}, ì˜¤ë””ì˜¤ì¶œë ¥: OFF)")
    
    async def heartbeat(self):
        """ì£¼ê¸°ì ì¸ heartbeat ì „ì†¡"""
        while self.is_connected:
            try:
                await asyncio.sleep(20)  # 20ì´ˆë§ˆë‹¤
                if self.openai_ws and self.is_connected:
                    await self.openai_ws.ping()
                    logger.debug("ğŸ’“ Heartbeat sent")
            except Exception as e:
                logger.error(f"âŒ Heartbeat ì˜¤ë¥˜: {e}")
                break
    
    async def send_audio(self, audio_base64: str):
        """ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡"""
        if not self.openai_ws or not self.is_connected:
            return
            
        try:
            # ğŸ“Š ì²« ì˜¤ë””ì˜¤ ì „ì†¡ ì‹œê° ê¸°ë¡
            if self.first_audio_time is None:
                self.first_audio_time = time.time()
                logger.info("ğŸ¤ ì²« ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡ ì‹œì‘")
            
            self.last_audio_time = time.time()
            
            event = {
                "type": "input_audio_buffer.append",
                "audio": audio_base64
            }
            await self.openai_ws.send(json.dumps(event))
            logger.debug("ğŸ“¤ ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡")
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ì „ì†¡ ì˜¤ë¥˜: {e}")
    
    async def commit_audio(self):
        """ì˜¤ë””ì˜¤ ë²„í¼ ì»¤ë°‹ (VAD ë¯¸ì‚¬ìš© ì‹œ)"""
        if not self.openai_ws or not self.is_connected:
            return
            
        try:
            event = {
                "type": "input_audio_buffer.commit"
            }
            await self.openai_ws.send(json.dumps(event))
            logger.debug("âœ… ì˜¤ë””ì˜¤ ë²„í¼ ì»¤ë°‹")
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë””ì˜¤ ì»¤ë°‹ ì˜¤ë¥˜: {e}")
    
    async def listen_openai_events(self):
        """OpenAI ì´ë²¤íŠ¸ ìˆ˜ì‹  ë° ì²˜ë¦¬"""
        try:
            async for message in self.openai_ws:
                try:
                    event = json.loads(message)
                    await self.handle_openai_event(event)
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                except Exception as e:
                    logger.error(f"âŒ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    
        except websockets.exceptions.ConnectionClosed:
            logger.info("ğŸ”Œ OpenAI ì—°ê²° ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì´ë²¤íŠ¸ ìˆ˜ì‹  ì˜¤ë¥˜: {e}")
        finally:
            self.is_connected = False
    
    async def handle_openai_event(self, event: dict):
        """OpenAI ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = event.get("type")
        
        if event_type == "session.created":
            self.session_id = event.get("session", {}).get("id")
            logger.info(f"ğŸ‰ ì„¸ì…˜ ìƒì„±ë¨: {self.session_id}")
            await self.client_ws.send_json({
                "type": "info",
                "message": f"âœ… Realtime API ì—°ê²° ì™„ë£Œ (ì„¸ì…˜: {self.session_id})"
            })
        
        elif event_type == "session.updated":
            logger.info("âš™ï¸ ì„¸ì…˜ ì—…ë°ì´íŠ¸ë¨")
        
        elif event_type == "conversation.item.input_audio_transcription.delta":
            # ì‹¤ì‹œê°„ ì „ì‚¬ ë¸íƒ€ (ë¶€ë¶„ ê²°ê³¼)
            delta = event.get("delta", "")
            transcript_id = event.get("item_id", "")
            
            # ğŸ“Š ì²« ì‘ë‹µ ì‹œê°„ ì¸¡ì •
            if delta and self.first_response_time is None and self.first_audio_time is not None:
                self.first_response_time = time.time()
                latency = self.first_response_time - self.first_audio_time
                logger.info(f"âš¡ ì²« ì‘ë‹µ ì§€ì—°ì‹œê°„: {latency:.2f}ì´ˆ (ì²« ì˜¤ë””ì˜¤ â†’ ì²« ë¸íƒ€)")
            
            if delta:
                logger.info(f"ğŸ“ [DELTA] {delta}")
                await self.client_ws.send_json({
                    "type": "transcript_delta",
                    "text": delta,
                    "item_id": transcript_id,
                    "timestamp": datetime.utcnow().isoformat()
                })
        
        elif event_type == "conversation.item.input_audio_transcription.completed":
            # ì „ì‚¬ ì™„ë£Œ (ìµœì¢… ê²°ê³¼)
            transcript = event.get("transcript", "")
            transcript_id = event.get("item_id", "")
            
            if transcript:
                # ğŸ“Š ì „ì‚¬ ì™„ë£Œ ì‹œê°„ ê¸°ë¡
                completion_time = time.time()
                self.transcripts.append({
                    "time": completion_time,
                    "text": transcript,
                    "id": transcript_id
                })
                
                # ë°œí™” í›„ ì „ì‚¬ê¹Œì§€ ê±¸ë¦° ì‹œê°„ (ë§ˆì§€ë§‰ ì˜¤ë””ì˜¤ ê¸°ì¤€)
                if self.last_audio_time:
                    time_since_last_audio = completion_time - self.last_audio_time
                    logger.info(f"â±ï¸  ë°œí™” ì¢…ë£Œ â†’ ì „ì‚¬ ì™„ë£Œ: {time_since_last_audio:.2f}ì´ˆ")
                
                # í‰ê·  ì „ì‚¬ ì‹œê°„ ê³„ì‚°
                if len(self.transcripts) >= 2:
                    intervals = []
                    for i in range(1, len(self.transcripts)):
                        interval = self.transcripts[i]["time"] - self.transcripts[i-1]["time"]
                        intervals.append(interval)
                    avg_interval = sum(intervals) / len(intervals)
                    logger.info(f"ğŸ“Š í‰ê·  ì „ì‚¬ ê°„ê²©: {avg_interval:.2f}ì´ˆ (ì „ì‚¬ {len(self.transcripts)}ê°œ)")
                
                logger.info(f"âœ… [COMPLETED] {transcript}")
                await self.client_ws.send_json({
                    "type": "transcript_completed",
                    "text": transcript,
                    "item_id": transcript_id,
                    "timestamp": datetime.utcnow().isoformat()
                })
        
        elif event_type == "conversation.item.input_audio_transcription.failed":
            error = event.get("error", {})
            logger.error(f"âŒ ì „ì‚¬ ì‹¤íŒ¨: {error}")
            await self.client_ws.send_json({
                "type": "error",
                "message": f"ì „ì‚¬ ì‹¤íŒ¨: {error.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            })
        
        elif event_type == "input_audio_buffer.speech_started":
            logger.info("ğŸ¤ ìŒì„± ê°ì§€ ì‹œì‘ (VAD)")
            await self.client_ws.send_json({
                "type": "speech_started",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        elif event_type == "input_audio_buffer.speech_stopped":
            logger.info("â¸ï¸ ìŒì„± ê°ì§€ ì¢…ë£Œ (VAD)")
            
            # ğŸ“Š ìŒì„± ì¢…ë£Œ ì‹œ ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
            if self.first_audio_time and self.last_audio_time:
                total_duration = self.last_audio_time - self.first_audio_time
                logger.info(f"ğŸ“Š === ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì„±ëŠ¥ ìš”ì•½ ===")
                logger.info(f"  ğŸ“ ì´ ë°œí™” ì‹œê°„: {total_duration:.2f}ì´ˆ")
                if self.first_response_time:
                    logger.info(f"  âš¡ ì²« ì‘ë‹µ ì§€ì—°: {self.first_response_time - self.first_audio_time:.2f}ì´ˆ")
                logger.info(f"  ğŸ“ ì „ì‚¬ íšŸìˆ˜: {len(self.transcripts)}ê°œ")
                logger.info(f"ğŸ“Š ============================")
            
            await self.client_ws.send_json({
                "type": "speech_stopped",
                "timestamp": datetime.utcnow().isoformat()
            })
        
        elif event_type == "input_audio_buffer.committed":
            logger.debug("âœ… ì˜¤ë””ì˜¤ ë²„í¼ ì»¤ë°‹ë¨")
        
        elif event_type == "error":
            error = event.get("error", {})
            logger.error(f"âŒ OpenAI ì˜¤ë¥˜: {error}")
            await self.client_ws.send_json({
                "type": "error",
                "message": f"OpenAI ì˜¤ë¥˜: {error.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            })
        
        else:
            logger.debug(f"ğŸ“¨ ê¸°íƒ€ ì´ë²¤íŠ¸: {event_type}")
    
    async def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        self.is_connected = False
        
        if self.heartbeat_task:
            self.heartbeat_task.cancel()
        
        if self.openai_ws:
            await self.openai_ws.close()
            logger.info("ğŸ”Œ OpenAI ì—°ê²° ì¢…ë£Œë¨")
